{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for training Spiking CNN on subset of NMNIST digits\n",
    "\n",
    "## The problem:\n",
    "Training digit classifier(0-9) on a subset(1000 training and 100 testing) of NMNIST digit spikes recorded using DVS camera. Just chagne the training list to for full NMNIST training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load proper paths for SLAYER Pytorch source modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "CURRENT_TEST_DIR = os.getcwd()\n",
    "sys.path.append(CURRENT_TEST_DIR + \"/../src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required modules\n",
    "\n",
    "SLAYER modules are available as `snn`\n",
    "* The `spike-layer` module will be available as `snn.layer`.\n",
    "* The `yaml-parameter` module will be availabe as `snn.params`.\n",
    "* The `spike-loss` module will be available as `snn.loss`.\n",
    "* The `spike-classifier` module will be available as `snn.predict`.\n",
    "* The `spike-IO` module will be available as `snn.io`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import slayerSNN as snn\n",
    "from learningStats import learningStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read SNN configuration from yaml file\n",
    "See the file for all the configuration parameters. This configuration file will be used to describe the SNN. We will ignore the network configuration  describe in the yaml file here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netParams = snn.params('network.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defne the dataset class\n",
    "The dataset definition follows standard PyTorch dataset definition.\n",
    "Internally, it utilizes snn.io modules to read spikes and returns the spike in correct tensor format (CHWT).\n",
    "* `datasetPath`: the path where the spike files are stored.\n",
    "* `sampleFile`: the file that contains a list of sample indices and its corresponding clases.\n",
    "* `samplingTime`: the sampling time (in ms) to bin the spikes.\n",
    "* `sampleLength`: the length of the sample (in ms)\n",
    "\n",
    "Note: This is a simple dataset class. A dataset that utilizes the folder hierarchy or xml list is easy to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset definition\n",
    "class nmnistDataset(Dataset):\n",
    "\tdef __init__(self, datasetPath, sampleFile, samplingTime, sampleLength):\n",
    "\t\tself.path = datasetPath \n",
    "\t\tself.samples = np.loadtxt(sampleFile).astype('int')\n",
    "\t\tself.samplingTime = samplingTime\n",
    "\t\tself.nTimeBins    = int(sampleLength / samplingTime)\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tinputIndex  = self.samples[index, 0]\n",
    "\t\tclassLabel  = self.samples[index, 1]\n",
    "\t\t\n",
    "\t\tinputSpikes = snn.io.read2Dspikes(\n",
    "\t\t\t\t\t\tself.path + str(inputIndex.item()) + '.bs2'\n",
    "\t\t\t\t\t\t).toSpikeTensor(torch.zeros((2,34,34,self.nTimeBins)),\n",
    "\t\t\t\t\t\tsamplingTime=self.samplingTime)\n",
    "\t\tdesiredClass = torch.zeros((10, 1, 1, 1))\n",
    "\t\tdesiredClass[classLabel,...] = 1\n",
    "\t\treturn inputSpikes, desiredClass, classLabel\n",
    "    \n",
    "\tdef __len__(self):\n",
    "\t\treturn self.samples.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network\n",
    "The network definition follows similar style as standard PyTorch network definition, but it utilizes snn modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "\tdef __init__(self, netParams):\n",
    "\t\tsuper(Network, self).__init__()\n",
    "\t\t# initialize slayer\n",
    "\t\tslayer = snn.layer(netParams['neuron'], netParams['simulation'])\n",
    "\t\tself.slayer = slayer\n",
    "\t\t# define network functions\n",
    "\t\tself.conv1 = slayer.conv(2, 16, 5, padding=1)\n",
    "\t\tself.conv2 = slayer.conv(16, 32, 3, padding=1)\n",
    "\t\tself.conv3 = slayer.conv(32, 64, 3, padding=1)\n",
    "\t\tself.pool1 = slayer.pool(2)\n",
    "\t\tself.pool2 = slayer.pool(2)\n",
    "\t\tself.fc1   = slayer.dense((8, 8, 64), 10)\n",
    "\n",
    "\tdef forward(self, spikeInput):\n",
    "\t\tspikeLayer1 = self.slayer.spike(self.conv1(self.slayer.psp(spikeInput ))) # 32, 32, 16\n",
    "\t\tspikeLayer2 = self.slayer.spike(self.pool1(self.slayer.psp(spikeLayer1))) # 16, 16, 16\n",
    "\t\tspikeLayer3 = self.slayer.spike(self.conv2(self.slayer.psp(spikeLayer2))) # 16, 16, 32\n",
    "\t\tspikeLayer4 = self.slayer.spike(self.pool2(self.slayer.psp(spikeLayer3))) #  8,  8, 32\n",
    "\t\tspikeLayer5 = self.slayer.spike(self.conv3(self.slayer.psp(spikeLayer4))) #  8,  8, 64\n",
    "\t\tspikeOut    = self.slayer.spike(self.fc1  (self.slayer.psp(spikeLayer5))) #  10\n",
    "\n",
    "\t\treturn spikeOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the network\n",
    "* Define the device to run the code on.\n",
    "* Create network instance.\n",
    "* Create loss instance.\n",
    "* Define optimizer module.\n",
    "* Define training and testing dataloader.\n",
    "* Cereate instance for learningStats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cuda device to run the code on.\n",
    "# device = torch.device('cuda')\n",
    "# Use multiple GPU's if available\n",
    "device = torch.device('cuda:2') # should be the first GPU of deviceIDs\n",
    "deviceIds = [2, 3]\n",
    "\n",
    "# Create network instance.\n",
    "# net = Network(netParams).to(device)\n",
    "# Split the network to run over multiple GPUs\n",
    "net = torch.nn.DataParallel(Network(netParams).to(device), device_ids=deviceIds)\n",
    "\n",
    "# Create snn loss instance.\n",
    "error = snn.loss(netParams).to(device)\n",
    "\n",
    "# Define optimizer module.\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.01, amsgrad = True)\n",
    "\n",
    "# Dataset and dataLoader instances.\n",
    "trainingSet = nmnistDataset(datasetPath =netParams['training']['path']['in'], \n",
    "\t\t\t\t\t\t    sampleFile  =netParams['training']['path']['train'],\n",
    "\t\t\t\t\t\t    samplingTime=netParams['simulation']['Ts'],\n",
    "\t\t\t\t\t\t    sampleLength=netParams['simulation']['tSample'])\n",
    "trainLoader = DataLoader(dataset=trainingSet, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "testingSet = nmnistDataset(datasetPath  =netParams['training']['path']['in'], \n",
    "\t\t\t\t\t\t    sampleFile  =netParams['training']['path']['test'],\n",
    "\t\t\t\t\t\t    samplingTime=netParams['simulation']['Ts'],\n",
    "\t\t\t\t\t\t    sampleLength=netParams['simulation']['tSample'])\n",
    "testLoader = DataLoader(dataset=testingSet, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "# Learning stats instance.\n",
    "stats = learningStats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the spike data\n",
    "Visualize the first five samples of the dataset.\n",
    "\n",
    "`snn.io.showTD` should show an animation of the sequence in normal python session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "\tinput, target, label = trainingSet[i]\n",
    "\tsnn.io.showTD(snn.io.spikeArrayToEvent(input.reshape((2, 34, 34, -1)).cpu().data.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "Train the network for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "\t# Reset training stats.\n",
    "\tstats.training.reset()\n",
    "\ttSt = datetime.now()\n",
    "\t\n",
    "\t# Training loop.\n",
    "\tfor i, (input, target, label) in enumerate(trainLoader, 0):\n",
    "\t\t# Move the input and target to correct GPU.\n",
    "\t\tinput  = input.to(device)\n",
    "\t\ttarget = target.to(device) \n",
    "        \n",
    "\t\t# Forward pass of the network.\n",
    "\t\toutput = net.forward(input)\n",
    "\t\t\n",
    "\t\t# Gather the training stats.\n",
    "\t\tstats.training.correctSamples += torch.sum( snn.predict.getClass(output) == label ).data.item()\n",
    "\t\tstats.training.numSamples     += len(label)\n",
    "        \n",
    "\t\t# Calculate loss.\n",
    "\t\tloss = error.numSpikes(output, target)\n",
    "        \n",
    "\t\t# Reset gradients to zero.\n",
    "\t\toptimizer.zero_grad()\n",
    "        \n",
    "\t\t# Backward pass of the network.\n",
    "\t\tloss.backward()\n",
    "        \n",
    "\t\t# Update weights.\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\t# Gather training loss stats.\n",
    "\t\tstats.training.lossSum += loss.cpu().data.item()\n",
    "\n",
    "\t\t# Display training stats. (Suitable for normal python implementation)\n",
    "\t\t# stats.print(epoch, i, (datetime.now() - tSt).total_seconds())\n",
    "\t\n",
    "    # Update training stats.\n",
    "\tstats.training.update()\n",
    "\t# Reset testing stats.\n",
    "\tstats.testing.reset()\n",
    "\t\n",
    "\t# Testing loop.\n",
    "\t# Same steps as Training loops except loss backpropagation and weight update.\n",
    "\tfor i, (input, target, label) in enumerate(testLoader, 0):\n",
    "\t\tinput  = input.to(device)\n",
    "\t\ttarget = target.to(device) \n",
    "\t\t\n",
    "\t\toutput = net.forward(input)\n",
    "\n",
    "\t\tstats.testing.correctSamples += torch.sum( snn.predict.getClass(output) == label ).data.item()\n",
    "\t\tstats.testing.numSamples     += len(label)\n",
    "\n",
    "\t\tloss = error.numSpikes(output, target)\n",
    "\t\tstats.testing.lossSum += loss.cpu().data.item()\n",
    "\t\t# stats.print(epoch, i)\n",
    "\t\n",
    "\t# Update testing stats.\n",
    "\tstats.testing.update()\n",
    "\tif epoch%10==0:  stats.print(epoch, timeElapsed=(datetime.now() - tSt).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.semilogy(stats.training.lossLog, label='Training')\n",
    "plt.semilogy(stats.testing .lossLog, label='Testing')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(stats.training.accuracyLog, label='Training')\n",
    "plt.plot(stats.testing .accuracyLog, label='Testing')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
